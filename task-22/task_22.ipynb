{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the performance and applicability of learnable positional encoding versus relative positional encoding in the context of audio processing, in the Transformer architecture. Write a script for both architectures, where you change this component of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchaudio\n",
    "from torchaudio.transforms import MelSpectrogram\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalAttention(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, window_size):\n",
    "        super(LocalAttention, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_size, num_heads)\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, embed_size = x.size()\n",
    "        output = torch.zeros_like(x)\n",
    "        for i in range(0, seq_len, self.window_size):\n",
    "            end = min(i + self.window_size, seq_len)\n",
    "            attn_output, _ = self.attention(x[:, i:end, :], x[:, i:end, :], x[:, i:end, :])\n",
    "            output[:, i:end, :] = attn_output\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnablePositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_size, max_len=5000):\n",
    "        super(LearnablePositionalEncoding, self).__init__()\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, max_len, embed_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.positional_encoding[:, :seq_len, :]\n",
    "\n",
    "class TransformerLearnablePositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, num_layers, window_size):\n",
    "        super(TransformerLearnablePositionalEncoding, self).__init__()\n",
    "        self.embedding = nn.Linear(128, embed_size)\n",
    "        self.positional_encoding = LearnablePositionalEncoding(embed_size)\n",
    "        self.local_attention_layers = nn.ModuleList(\n",
    "            [LocalAttention(embed_size, num_heads, window_size) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.fc = nn.Linear(embed_size, 10)  # Example output size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        for local_layer in self.local_attention_layers:\n",
    "            x = local_layer(x)\n",
    "        x = self.fc(x.mean(dim=1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativePositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_size, max_len=5000):\n",
    "        super(RelativePositionalEncoding, self).__init__()\n",
    "        self.relative_positions = nn.Parameter(torch.zeros(max_len, embed_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        relative_positions = self.relative_positions[:seq_len, :]\n",
    "        return x + relative_positions.unsqueeze(0)\n",
    "\n",
    "class TransformerRelativePositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, num_layers, window_size):\n",
    "        super(TransformerRelativePositionalEncoding, self).__init__()\n",
    "        self.embedding = nn.Linear(128, embed_size)\n",
    "        self.positional_encoding = RelativePositionalEncoding(embed_size)\n",
    "        self.local_attention_layers = nn.ModuleList(\n",
    "            [LocalAttention(embed_size, num_heads, window_size) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.fc = nn.Linear(embed_size, 10)  # Example output size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        for local_layer in self.local_attention_layers:\n",
    "            x = local_layer(x)\n",
    "        x = self.fc(x.mean(dim=1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets():\n",
    "    # Ensure the data directory exists\n",
    "    if not os.path.exists('data'):\n",
    "        os.makedirs('data')\n",
    "\n",
    "    # Remove any partially downloaded files\n",
    "    partial_files = [f for f in os.listdir('data') if f.endswith('.partial')]\n",
    "    for f in partial_files:\n",
    "        os.remove(os.path.join('data', f))\n",
    "\n",
    "    # Download and load the dataset with error handling\n",
    "    try:\n",
    "        train_dataset = torchaudio.datasets.LIBRISPEECH(root=\"data\", url=\"train-clean-100\", download=True)\n",
    "        test_dataset = torchaudio.datasets.LIBRISPEECH(root=\"data\", url=\"test-clean\", download=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading dataset: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "    # Split the training dataset into training and validation sets\n",
    "    train_size = int(0.8 * len(train_dataset))\n",
    "    val_size = len(train_dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.mel_spectrogram = MelSpectrogram()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        waveform, sample_rate, _, _, label, _ = self.dataset[idx]\n",
    "        mel_spectrogram = self.mel_spectrogram(waveform)\n",
    "        mel_spectrogram = mel_spectrogram.permute(0, 2, 1)  # (batch_size, seq_len, feature_dim)\n",
    "        return mel_spectrogram, label\n",
    "\n",
    "def train_model(model, dataloader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        epoch_loss = running_loss / len(dataloader.dataset)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Prepare the datasets\n",
    "    train_dataset, val_dataset, test_dataset = prepare_datasets()\n",
    "    if train_dataset is None or val_dataset is None or test_dataset is None:\n",
    "        print(\"Failed to prepare datasets.\")\n",
    "        exit(1)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_dataloader = DataLoader(AudioDataset(train_dataset), batch_size=32, shuffle=True)\n",
    "    val_dataloader = DataLoader(AudioDataset(val_dataset), batch_size=32, shuffle=False)\n",
    "    test_dataloader = DataLoader(AudioDataset(test_dataset), batch_size=32, shuffle=False)\n",
    "\n",
    "    # Initialize the models, criterion, and optimizer\n",
    "    model_learnable = TransformerLearnablePositionalEncoding(embed_size=256, num_heads=8, num_layers=4, window_size=10)\n",
    "    model_relative = TransformerRelativePositionalEncoding(embed_size=256, num_heads=8, num_layers=4, window_size=10)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer_learnable = optim.Adam(model_learnable.parameters(), lr=0.001)\n",
    "    optimizer_relative = optim.Adam(model_relative.parameters(), lr=0.001)\n",
    "\n",
    "    # Train the learnable positional encoding model\n",
    "    print(\"Training Learnable Positional Encoding Model\")\n",
    "    train_model(model_learnable, train_dataloader, criterion, optimizer_learnable, num_epochs=10)\n",
    "    accuracy_learnable, precision_learnable, recall_learnable, f1_learnable = evaluate_model(model_learnable, test_dataloader)\n",
    "    print(f'Learnable Positional Encoding Model - Accuracy: {accuracy_learnable:.4f}, Precision: {precision_learnable:.4f}, Recall: {recall_learnable:.4f}, F1 Score: {f1_learnable:.4f}')\n",
    "\n",
    "    # Train the relative positional encoding model\n",
    "    print(\"Training Relative Positional Encoding Model\")\n",
    "    train_model(model_relative, train_dataloader, criterion, optimizer_relative, num_epochs=10)\n",
    "    accuracy_relative, precision_relative, recall_relative, f1_relative = evaluate_model(model_relative, test_dataloader)\n",
    "    print(f'Relative Positional Encoding Model - Accuracy: {accuracy_relative:.4f}, Precision: {precision_relative:.4f}, Recall: {recall_relative:.4f}, F1 Score: {f1_relative:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
