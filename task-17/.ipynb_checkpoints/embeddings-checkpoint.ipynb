{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b1e6bba-c018-4dea-a13f-64df1e5c3bc9",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "/home/kennykguo/anaconda3/envs/deep-learning/lib/python3.8/site-packages/torchaudio/lib/libtorchaudio.so: undefined symbol: _ZN3c1010Dispatcher17runRecordFunctionERN2at14RecordFunctionESt17reference_wrapperIKNS_14FunctionSchemaEENS_11DispatchKeyE",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchaudio\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchaudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mT\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Wav2Vec2Processor, Wav2Vec2Model, HubertProcessor, HubertModel\n",
      "File \u001b[0;32m~/anaconda3/envs/deep-learning/lib/python3.8/site-packages/torchaudio/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize extension and backend first\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _extension  \u001b[38;5;66;03m# noqa  # usort: skip\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_backend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa  # usort: skip\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     AudioMetaData,\n\u001b[1;32m      5\u001b[0m     get_audio_backend,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     set_audio_backend,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     compliance,\n\u001b[1;32m     15\u001b[0m     datasets,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     utils,\n\u001b[1;32m     24\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/deep-learning/lib/python3.8/site-packages/torchaudio/_extension/__init__.py:38\u001b[0m\n\u001b[1;32m     36\u001b[0m _IS_ALIGN_AVAILABLE \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _IS_TORCHAUDIO_EXT_AVAILABLE:\n\u001b[0;32m---> 38\u001b[0m     \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlibtorchaudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchaudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_torchaudio\u001b[39;00m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     _check_cuda_version()\n",
      "File \u001b[0;32m~/anaconda3/envs/deep-learning/lib/python3.8/site-packages/torchaudio/_extension/utils.py:60\u001b[0m, in \u001b[0;36m_load_lib\u001b[0;34m(lib)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/deep-learning/lib/python3.8/site-packages/torch/_ops.py:1295\u001b[0m, in \u001b[0;36m_Ops.load_library\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m   1290\u001b[0m path \u001b[38;5;241m=\u001b[39m _utils_internal\u001b[38;5;241m.\u001b[39mresolve_library_path(path)\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dl_open_guard():\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;66;03m# Import the shared library into the process, thus running its\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;66;03m# static (global) initialization code in order to register custom\u001b[39;00m\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;66;03m# operators with the JIT.\u001b[39;00m\n\u001b[0;32m-> 1295\u001b[0m     \u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCDLL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloaded_libraries\u001b[38;5;241m.\u001b[39madd(path)\n",
      "File \u001b[0;32m~/anaconda3/envs/deep-learning/lib/python3.8/ctypes/__init__.py:373\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 373\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[0;31mOSError\u001b[0m: /home/kennykguo/anaconda3/envs/deep-learning/lib/python3.8/site-packages/torchaudio/lib/libtorchaudio.so: undefined symbol: _ZN3c1010Dispatcher17runRecordFunctionERN2at14RecordFunctionESt17reference_wrapperIKNS_14FunctionSchemaEENS_11DispatchKeyE"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model, HubertProcessor, HubertModel\n",
    "from speechbrain.pretrained import EncoderASR\n",
    "from performer_pytorch import Performer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97593aea-de91-4f9a-a275-62f3bf7f2f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "AUDIO_FILE = \"example.wav\"\n",
    "SAMPLE_RATE = 16000\n",
    "N_FFT = 400\n",
    "HOP_LENGTH = 160\n",
    "N_MELS = 80\n",
    "N_MFCC = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a70a17-f6fc-434a-aee4-808f0a141fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_waveform(waveform, sr):\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.plot(waveform.numpy().T)\n",
    "    plt.title(\"Raw Waveform\")\n",
    "    plt.xlabel(\"Samples\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_spectrogram(spec, title, ylabel):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.imshow(spec.log2().squeeze().numpy(), aspect='auto', origin='lower')\n",
    "    plt.title(title)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xlabel(\"Frame\")\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_embeddings(embeddings, title):\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced = pca.fit_transform(embeddings.squeeze().cpu().numpy())\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(reduced[:, 0], reduced[:, 1], alpha=0.5)\n",
    "    plt.title(title + \" (PCA Reduced)\")\n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe26798-a4c4-4687-a61a-db8735aaac76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess audio\n",
    "waveform, orig_sr = torchaudio.load(AUDIO_FILE)\n",
    "resampler = T.Resample(orig_sr, SAMPLE_RATE)\n",
    "waveform = resampler(waveform)\n",
    "\n",
    "print(\"\\nRaw audio shape:\", waveform.shape)\n",
    "plot_waveform(waveform, SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b18620cb-2758-4caa-afc9-474abd43a85c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'T' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Feature extraction\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m mel_spec \u001b[38;5;241m=\u001b[39m \u001b[43mT\u001b[49m\u001b[38;5;241m.\u001b[39mMelSpectrogram(\n\u001b[1;32m      3\u001b[0m     sample_rate\u001b[38;5;241m=\u001b[39mSAMPLE_RATE,\n\u001b[1;32m      4\u001b[0m     n_fft\u001b[38;5;241m=\u001b[39mN_FFT,\n\u001b[1;32m      5\u001b[0m     hop_length\u001b[38;5;241m=\u001b[39mHOP_LENGTH,\n\u001b[1;32m      6\u001b[0m     n_mels\u001b[38;5;241m=\u001b[39mN_MELS\n\u001b[1;32m      7\u001b[0m )(waveform)\n\u001b[1;32m      9\u001b[0m mfcc \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mMFCC(\n\u001b[1;32m     10\u001b[0m     sample_rate\u001b[38;5;241m=\u001b[39mSAMPLE_RATE,\n\u001b[1;32m     11\u001b[0m     n_mfcc\u001b[38;5;241m=\u001b[39mN_MFCC,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     }\n\u001b[1;32m     17\u001b[0m )(waveform)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'T' is not defined"
     ]
    }
   ],
   "source": [
    "# Feature extraction\n",
    "mel_spec = T.MelSpectrogram(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    n_fft=N_FFT,\n",
    "    hop_length=HOP_LENGTH,\n",
    "    n_mels=N_MELS\n",
    ")(waveform)\n",
    "\n",
    "mfcc = T.MFCC(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    n_mfcc=N_MFCC,\n",
    "    melkwargs={\n",
    "        'n_fft': N_FFT,\n",
    "        'hop_length': HOP_LENGTH,\n",
    "        'n_mels': N_MELS\n",
    "    }\n",
    ")(waveform)\n",
    "\n",
    "\n",
    "print(\"\\nMel spectrogram shape:\", mel_spec.shape)\n",
    "plot_spectrogram(mel_spec, \"Mel Spectrogram\", \"Mel Bin\")\n",
    "\n",
    "print(\"\\nMFCC shape:\", mfcc.shape)\n",
    "plot_spectrogram(mfcc, \"MFCC\", \"Coefficient Index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485eefcb-b81d-4f7c-85b8-af20aed88a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"\\nMel spectrogram shape:\", mel_spec.shape)\n",
    "plot_spectrogram(mel_spec, \"Mel Spectrogram\", \"Mel Bin\")\n",
    "\n",
    "print(\"\\nMFCC shape:\", mfcc.shape)\n",
    "plot_spectrogram(mfcc, \"MFCC\", \"Coefficient Index\")\n",
    "\n",
    "# HuBERT Processing\n",
    "hubert_processor = HubertProcessor.from_pretrained(\"facebook/hubert-base-ls960\")\n",
    "hubert_model = HubertModel.from_pretrained(\"facebook/hubert-base-ls960\")\n",
    "\n",
    "hubert_inputs = hubert_processor(\n",
    "    waveform.squeeze().numpy(),\n",
    "    return_tensors=\"pt\",\n",
    "    sampling_rate=SAMPLE_RATE\n",
    ").input_values\n",
    "\n",
    "with torch.no_grad():\n",
    "    hubert_output = hubert_model(hubert_inputs).last_hidden_state\n",
    "\n",
    "print(\"\\nHuBERT output shape:\", hubert_output.shape)\n",
    "visualize_embeddings(hubert_output, \"HuBERT Embeddings\")\n",
    "\n",
    "# Wav2Vec 2.0 Processing\n",
    "w2v_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "w2v_model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "w2v_inputs = w2v_processor(\n",
    "    waveform.squeeze().numpy(),\n",
    "    return_tensors=\"pt\",\n",
    "    sampling_rate=SAMPLE_RATE\n",
    ").input_values\n",
    "\n",
    "with torch.no_grad():\n",
    "    w2v_output = w2v_model(w2v_inputs).last_hidden_state\n",
    "\n",
    "print(\"\\nWav2Vec 2.0 output shape:\", w2v_output.shape)\n",
    "visualize_embeddings(w2v_output, \"Wav2Vec 2.0 Embeddings\")\n",
    "\n",
    "# Conformer Processing (using SpeechBrain)\n",
    "conformer_model = EncoderASR.from_hparams(\n",
    "    source=\"speechbrain/asr-conformer-transformerlm-librispeech\",\n",
    "    savedir=\"pretrained_models/conformer\"\n",
    ")\n",
    "\n",
    "log_mel = torch.log(mel_spec + 1e-6).squeeze(0).transpose(0, 1)\n",
    "log_mel = (log_mel - log_mel.mean(dim=0)) / log_mel.std(dim=0)\n",
    "conformer_input = log_mel.unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    conformer_output = conformer_model.encode_batch(conformer_input)\n",
    "\n",
    "print(\"\\nConformer output shape:\", conformer_output.shape)\n",
    "visualize_embeddings(conformer_output, \"Conformer Embeddings\")\n",
    "\n",
    "# Performer Processing (example implementation)\n",
    "performer = Performer(\n",
    "    dim=64,\n",
    "    depth=4,\n",
    "    heads=8,\n",
    "    dim_head=64,\n",
    "    causal=True\n",
    ")\n",
    "\n",
    "performer_input = mel_spec.squeeze(0).transpose(0, 1).unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "    performer_output = performer(performer_input)\n",
    "\n",
    "print(\"\\nPerformer output shape:\", performer_output.shape)\n",
    "visualize_embeddings(performer_output, \"Performer Embeddings\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deep-learning)",
   "language": "python",
   "name": "deep-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
